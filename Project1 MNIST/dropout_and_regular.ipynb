{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Dropout and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. During training, a random set of neurons are \"dropped out\" or ignored with a certain probability, which forces the network to learn more robust features and reduces its reliance on any individual neuron. This helps to improve the generalization of the model and make it more resistant to noise in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BP Net as an example\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.dropout = nn.Dropout(0.5)    # Drop out 50% nodes randomly\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.activate = nn.ReLU()\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)    # Flatten the images into a vector\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In neural networks, regularization methods are used to prevent overfitting, which occurs when the model performs well on the training data but poorly on new, unseen data. Regularization techniques impose constraints on the network's complexity or modify the learning process to encourage simpler models that generalize better. Common regularization methods include L1 and L2 regularization, dropout, early stopping, and data augmentation. These methods help improve the model's ability to generalize from the training data to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization can achieve the effect of sparse model parameters.\n",
    "\n",
    "$$\n",
    "C = C_0 + \\frac{\\lambda}{n}\\sum_{w}|w|,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization can make the weight of the model decay, so that the model parameter values are close to 0.\n",
    "\n",
    "$$\n",
    "C = C_0 + \\frac{\\lambda}{2n} \\sum_{w}w^2,\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "C_0 \\text{ --- original loss function,}\\\\\n",
    "n \\text{ --- number of samples,}\\\\\n",
    "\\lambda \\text{ --- coefficient of regularization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001) # Set L2 regulariation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
